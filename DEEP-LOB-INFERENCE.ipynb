{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5dce70",
   "metadata": {},
   "source": [
    "**DEEPLOB Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31508584",
   "metadata": {},
   "source": [
    "**Reference:** DEEPLOB-XBTUSD.ipynb in exocharts github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65607f6",
   "metadata": {},
   "source": [
    "**Description:** This is inference notebook of DEEPLOB model. Due to the training being so hard and time/energy consuming, I had to outsource the training of the model to a different machine in the cloud. I trained a few models, especially changing the row count to reflect if I need to use all of the data.\n",
    "The test data was made from full data (last 25%), so that no model could have seen that data because they only saw n rows of dataset beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55d65ba-19bb-483b-acd9-00749734525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255169a6-cc46-42dc-b60d-bbafe4141769",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.load(\"xtest.npy\")\n",
    "ytest = np.load(\"ytest.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0f8d17-d24b-4e56-94dc-58ce636c786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((876569, 100, 40, 1), (876569, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317c1fd3-91d9-4468-8f6b-f68e3012964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, MaxPooling2D, concatenate, LSTM, Reshape, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics):\n",
    "    \n",
    "    input_tensor = Input(shape=(lookback_timestep, feature_num, 1))\n",
    "    \n",
    "    conv_layer1 = Conv2D(conv_filter_num, (1,2), strides=(1, 2))(input_tensor)\n",
    "    conv_layer1 =LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "    conv_layer1 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer1)\n",
    "    conv_first1 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "    conv_layer1 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer1)\n",
    "    conv_layer1 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer1)\n",
    "\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (1,2), strides=(1, 2))(conv_layer1)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer2)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "    conv_layer2 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer2)\n",
    "    conv_layer2 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer2)\n",
    "\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (1,10))(conv_layer2)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer3)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    conv_layer3 = Conv2D(conv_filter_num, (4,1), padding='same')(conv_layer3)\n",
    "    conv_layer3 = LeakyReLU(alpha=leaky_relu_alpha)(conv_layer3)\n",
    "    \n",
    "    inception_module1 = Conv2D(inception_num, (1,1), padding='same')(conv_layer3)\n",
    "    inception_module1 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module1)\n",
    "    inception_module1 = Conv2D(inception_num, (3,1), padding='same')(inception_module1)\n",
    "    inception_module1 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module1)\n",
    "\n",
    "    inception_module2 = Conv2D(inception_num, (1,1), padding='same')(conv_layer3)\n",
    "    inception_module2 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module2)\n",
    "    inception_module2 = Conv2D(inception_num, (5,1), padding='same')(inception_module2)\n",
    "    inception_module2 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module2)\n",
    "\n",
    "    inception_module3 = MaxPooling2D((3,1), strides=(1,1), padding='same')(conv_layer3)\n",
    "    inception_module3 = Conv2D(inception_num, (1,1), padding='same')(inception_module3)\n",
    "    inception_module3 = LeakyReLU(alpha=leaky_relu_alpha)(inception_module3)\n",
    "    \n",
    "    inception_module_final = concatenate([inception_module1, inception_module2, inception_module3], axis=3)\n",
    "    inception_module_final = Reshape((inception_module_final.shape[1], inception_module_final.shape[3]))(inception_module_final)\n",
    "\n",
    "    LSTM_output = LSTM(LSTM_num)(inception_module_final)\n",
    "\n",
    "    model_output = Dense(3, activation='softmax')(LSTM_output)\n",
    "    \n",
    "    DeepLOB_model = Model(inputs=input_tensor, outputs= model_output)  \n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n",
    "    \n",
    "    DeepLOB_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return DeepLOB_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f399bb-9b4b-457c-bbbe-0bb9ea2e3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_timestep = 100\n",
    "feature_num = 40\n",
    "conv_filter_num = 16\n",
    "inception_num = 32\n",
    "LSTM_num = 64\n",
    "leaky_relu_alpha = 0.01\n",
    "loss = 'categorical_crossentropy'\n",
    "learning_rate = 0.01\n",
    "adam_epsilon = 1\n",
    "optimizer = Adam(learning_rate=learning_rate, epsilon=1)\n",
    "batch_size = 32\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26feebdd-deb5-42c4-af62-716d5723f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:21:25.960714: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 14025104000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27393/27393 [==============================] - 821s 30ms/step - loss: 2.7297 - accuracy: 0.3604\n"
     ]
    }
   ],
   "source": [
    "model1 = initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics)\n",
    "model1.load_weights(\"10000nmodel.epoch140-loss0.40.hdf5\")\n",
    "loss1, acc1 = model1.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5000288c-ec48-4b86-9814-e60fe0ab9d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:35:20.540810: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 14025104000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27393/27393 [==============================] - 825s 30ms/step - loss: 1.9440 - accuracy: 0.4014\n"
     ]
    }
   ],
   "source": [
    "model2 = initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics)\n",
    "model2.load_weights(\"50000nmodel.epoch89-loss0.50.hdf5\")\n",
    "loss2, acc2 = model2.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee138a5-61ae-488f-9358-c1b157c7975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:49:21.546859: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 14025104000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24252/27393 [=========================>....] - ETA: 1:34 - loss: 1.2153 - accuracy: 0.4792"
     ]
    }
   ],
   "source": [
    "model3 = initiate_DeepLOB_model(lookback_timestep, feature_num, conv_filter_num, inception_num, LSTM_num, leaky_relu_alpha,\n",
    "                          loss, optimizer, metrics)\n",
    "model3.load_weights(\"allnmodel.epoch33-loss0.70.hdf5\")\n",
    "loss3, acc3 = model3.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6abce0",
   "metadata": {},
   "source": [
    "**Conclusion:** There seems to be a clear indication that with more data I give to the model, the bigger the accuracy is on the same test data, parameter tuning should be the next logical step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
